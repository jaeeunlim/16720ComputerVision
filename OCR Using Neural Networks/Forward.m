function [out, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.

n = length(W);
act_h = cell(1,n);
act_a = cell(1,n);

% First Hidden Layer
act_a{1} = W{1}*X' + b{1}; % pre-activation
act_h{1} = ((1+exp(-1*act_a{1})).^-1); % post-activation (sigmoid)

for i=2:n-1
    act_a{i} = W{i}*act_h{i-1} + b{i}; % pre-activation
    act_h{i} = ((1+exp(-1*act_a{i})).^-1); % post-activation (sigmoid)
end

% Output Layer
a = W{end}*act_h{end-1} + b{end}; % pre-activation
act_a{end} = a;
% post-activation (softmax)
den = 0;
for i=1:length(a)
    den = den + exp(a(i));
end
out = exp(a)/den;
act_h{end} = out;

end
